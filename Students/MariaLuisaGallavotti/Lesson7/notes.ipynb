{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes 27/03/2024\n",
    "## Sampling a PDF\n",
    "**• 1D sampling: cumulative distribution function**\n",
    "- *F*: cumulative function\n",
    "- *x*: values of my function *f*\n",
    "\n",
    "  -> sampling the inverse of the CDF I can obtain *f*: $F^{-1}(U)\\simeq f$\n",
    "\n",
    "**• Multivariate Gaussian Distribution**\n",
    "- $X=N(\\mu ,\\Sigma)$ --> $X=LZ+\\mu$ with $E(X)=\\mu$ and $\\Sigma=LL^t$\n",
    "\n",
    "**Rejection Sampling** *(it is a purely Monte Carlo method)*\n",
    "- the efficiency is proportional to the volume of the *target PDF* over the volume of the *proposal distribution*\n",
    "- every (generated) point is independent from the others, so I lose a lot of information\n",
    "\n",
    "**• MONTE CARLO MARKOV CHAIN**\n",
    "\n",
    "$P(\\theta | d)=\\frac{P(d|\\theta)P(\\theta)}{P(d)}$\n",
    "- posterior: $P(\\theta | d)$\n",
    "- likelihood: $P(d|\\theta)$\n",
    "- prior: $P(\\theta)$\n",
    "\n",
    "*The difference from before is that the procedure of extraction is exploiting the information that it has gathered during the previous steps.*\n",
    "\n",
    "This allows the process to have a very **high efficiency**.\n",
    "\n",
    "A classic algorithm for the Monte Carlo Markov Chain is the **MH Algorithm**:\n",
    "$\\frac{pdf(x_i)q(x_i)}{pdf(x_{i-1})q(x_{i-1})}>1$\n",
    "\n",
    "The *convergence* of the chain defines the *Burn_in* time, which is the time spent by the process forgetting about its initial state.\n",
    "\n",
    "The *Autocorrelation time* is the time spent by the autocorrelation getting to zero."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

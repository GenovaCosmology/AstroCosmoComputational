{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import emcee\n",
    "import corner\n",
    "\n",
    "# Define the dataset\n",
    "\n",
    "# Read the data from the file\n",
    "file_path = './lcparam_full_long.txt'\n",
    "df = pd.read_csv(file_path, delim_whitespace=True, comment='#', header=None, names=['name', 'zcmb', 'zhel', 'dz', 'mb', 'dmb', 'x1', 'dx1', 'color', 'dcolor', '3rdvar', 'd3rdvar', 'cov_m_s', 'cov_m_c', 'cov_s_c', 'set', 'ra', 'dec', 'biascor'])\n",
    "#in this file I don't put the supernovae with strange names (last supernovae)\n",
    "\n",
    "\n",
    "'''\n",
    "x1: SALT2 light curve stretch parameter.\n",
    "c: SALT2 light curve color parameter.\n",
    "mb: SALT2 light curve peak magnitude in B band.\n",
    "μCorr: Corrected distance modulus. \n",
    "Mass: Host galaxy stellar mass.\n",
    "zcmb: CMB frame redshift.\n",
    "zhel: Heliocentric redshift.\n",
    "'''\n",
    "\n",
    "# Extract the parameters we want to plot\n",
    "params = df[[\"zcmb\", \"mb\", \"dmb\", \"x1\",\"color\"]].values #.values converts the name of the columns to a numpy array\n",
    "\n",
    "# Define the log-likelihood function using the given formula\n",
    "#Explanation of the likelihood:\n",
    "'''\n",
    "d is the data vector--> params (in the following function)\n",
    "mu is the mean vector--> theta (in the following function)\n",
    "sigma_inv is the inverse of the covariance matrix\n",
    "'''\n",
    "def log_likelihood(theta, params, Sigma_inv):\n",
    "    mu = theta\n",
    "    ones = np.ones(params.shape[0])\n",
    "    \n",
    "    diff = params - mu\n",
    "    term1 = -0.5 * np.dot(np.dot(diff.T, Sigma_inv), diff)\n",
    "    \n",
    "    Sigma_inv_ones = np.dot(Sigma_inv, ones)\n",
    "    numerator = np.dot(ones.T, np.dot(Sigma_inv, diff)) ** 2\n",
    "    denominator = np.dot(ones.T, Sigma_inv_ones)\n",
    "    term2 = 0.5 * (numerator / denominator)\n",
    "    \n",
    "    return term1 + term2\n",
    "\n",
    "# Define the prior (assuming a uniform prior for simplicity)\n",
    "def log_prior(theta):\n",
    "    size_theta = len(theta)\n",
    "    #create a vector of zero with the dimension fo theta\n",
    "    zero_vector = np.zeros(size_theta)\n",
    "    hundred_vector = 100*np.ones(size_theta)\n",
    "    for i in range(size_theta):\n",
    "        if np.all(theta > zero_vector) and np.all(theta < hundred_vector):\n",
    "            return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "# Define the posterior probability\n",
    "#This is used to estimate the posterior distribution of the parameters given the data.\n",
    "def log_probability(theta, params, Sigma_inv):\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, params, Sigma_inv)\n",
    "\n",
    "# Covariance matrix and its inverse (assuming identity for simplicity, replace with actual covariance matrix)\n",
    "#for the sigma I put the error of mb: dmb\n",
    "Sigma = np.diag(df['dmb']**2)\n",
    "Sigma_inv = np.linalg.inv(Sigma)\n",
    "#print(log_likelihood([0.1, 0.1, 0.1, 0.1, 0.1], params, Sigma_inv))\n",
    "#print(log_probability([0.1, 0.1, 0.1, 0.1, 0.1], params, Sigma_inv))\n",
    "\n",
    "\n",
    "# Initialize the MCMC sampler\n",
    "nwalkers, ndim = 50, params.shape[1] #walkers are the number of chains and ndim is the number of parameters (nwalkers must be greater or equal than ndim)\n",
    "initial = params.mean(axis=0)\n",
    "pos = initial + 1e-4 * np.random.randn(nwalkers, ndim)\n",
    "#print(pos)\n",
    "#print(1e-4 * np.random.randn(nwalkers, ndim))\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability, args=(params, Sigma_inv))\n",
    "\n",
    "# Run the MCMC chain\n",
    "sampler.run_mcmc(pos, 5000, progress=True)\n",
    "'''\n",
    "# Get the samples and create the corner plot\n",
    "samples = sampler.get_chain(discard=100, thin=15, flat=True)\n",
    "\n",
    "# Create the corner plot\n",
    "figure = corner.corner(samples, labels=[\"zcmb\", \"mb\", \"dmb\"], truths=params.mean(axis=0))\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of the code \n",
    "* Summary of the code:\n",
    "\n",
    "We want to study the Cosmology of the Pantheon data (Redshift of the Supernovae).\n",
    "To make this we will create the likelihood, then with a Montecarlo process we sample the distribution and make a corner plot to understand what we found.\n",
    "\n",
    "\n",
    "* Explanation of get_mu:\n",
    "\n",
    "With this function we can define the Cosmology model. We need this function because in the likelihood we will use mu_model (mu is the distance modulus).\n",
    "\n",
    "\n",
    "\n",
    "* Explanation of the function relates to the likelihood:\n",
    "\n",
    "The likelihood function measures the probability of observing the data given a set of parameters 𝜃.\n",
    "\n",
    "The prior distribution respresents what I think about the parameters 𝜃 before observing data.\n",
    "\n",
    "The posterior distribution represents my thought plus the observing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emcee\n",
    "import corner\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_camb_path = os.path.join(os.path.dirname(os.path.abspath('.')), 'CAMB')\n",
    "sys.path.insert(0, _camb_path)\n",
    "\n",
    "import camb\n",
    "print('Using CAMB %s installed at %s' % (camb.__version__, os.path.dirname(camb.__file__)))\n",
    "\n",
    "# Load the data\n",
    "file_path = 'lcparam_full_long.txt'\n",
    "\n",
    "# Read the first line to get the header\n",
    "with open(file_path, 'r') as file:\n",
    "    header_line = file.readline().strip()\n",
    "    header = header_line.lstrip('#').strip()\n",
    "\n",
    "# Remove the last word from the header (assumed to be 'biascor')\n",
    "header_words = header.split()\n",
    "header_words.pop()\n",
    "header = ' '.join(header_words)\n",
    "\n",
    "# Read the rest of the file into a DataFrame\n",
    "data = pd.read_csv(file_path, sep='\\s+', skiprows=1, header=None)\n",
    "data.columns = header.split()\n",
    "\n",
    "# Extract relevant columns\n",
    "d = data['mb'].values\n",
    "dmb = data['dmb'].values\n",
    "\n",
    "# Compute the covariance matrix\n",
    "Sigma = np.diag(dmb**2)\n",
    "Sigma_inv = np.linalg.inv(Sigma)\n",
    "\n",
    "print(data.head())\n",
    "print(\"Covariance Matrix (Sigma):\")\n",
    "print(Sigma)\n",
    "print(\"Inverse Covariance Matrix (Sigma_inv):\")\n",
    "print(Sigma_inv)\n",
    "\n",
    "# Define the cosmological model, because later we will use it in the likelihood function\n",
    "def get_mu(z, H0, Om0, w0, wa):\n",
    "    pars = camb.CAMBparams()\n",
    "    pars.set_dark_energy(w=w0, wa=wa)\n",
    "    pars.set_cosmology(H0=H0, ombh2=0.022, omch2=Om0*0.12)\n",
    "    pars.InitPower.set_params(ns=0.965)\n",
    "    results = camb.get_results(pars)\n",
    "    \n",
    "    '''\n",
    "    # Debugging: Check if w crosses -1\n",
    "    z = np.array([0.1, 0.5, 1.0])  # Example redshifts\n",
    "    w = w0 + wa * (z / (1 + z))\n",
    "    if np.any(w > -1):\n",
    "        print(f\"w crosses -1 at redshifts: {z[w > -1]}\")\n",
    "    '''\n",
    "\n",
    "    # Get luminosity distance in Mpc\n",
    "    dl = results.comoving_radial_distance(z) * (1 + z)\n",
    "    \n",
    "    # Convert to distance modulus\n",
    "    mu = 5 * np.log10(dl) + 25 #The constant 25 is added to adjust the units and to match the conventional definitions used in astronomy -->\n",
    "    #--> for a luminosity distance of 10 Mpc, the distance modulus would be 30.\n",
    "    return mu\n",
    "\n",
    "def ln_likelihood(params, data, cov_inv):\n",
    "    H0, Om0, w0, wa = params\n",
    "    z = data['zcmb'].values\n",
    "    d = data['mb'].values\n",
    "    try:\n",
    "        mu = get_mu(z, H0, Om0, w0, wa)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_mu: {e}\")\n",
    "        return -np.inf\n",
    "    diff = d - mu\n",
    "    term1 = -0.5 * diff @ cov_inv @ diff # @ is the matrix multiplication\n",
    "    term2 = 0.5 * ((np.ones_like(diff) @ cov_inv @ diff) ** 2) / (np.ones_like(diff) @ cov_inv @ np.ones_like(diff))\n",
    "    return term1 + term2\n",
    "\n",
    "def log_prior(params):\n",
    "    H0, Om0, w0, wa = params\n",
    "    if not (60 < H0 < 80):\n",
    "        return -np.inf\n",
    "    if not (0.2 < Om0 < 0.4):\n",
    "        return -np.inf\n",
    "    if not (-1. < w0 < -0.):\n",
    "        return -np.inf\n",
    "    if not (-2. < wa < -0.5):\n",
    "        return -np.inf\n",
    "    \n",
    "    # Constraint to avoid crossing w = -1 (relaxed)\n",
    "    '''\n",
    "    if w0 + wa < -1:\n",
    "        return -np.inf\n",
    "    '''\n",
    "    return 0.0\n",
    "\n",
    "def log_posterior(params, data, cov_inv):\n",
    "    lp = log_prior(params)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + ln_likelihood(params, data, cov_inv)\n",
    "\n",
    "# Initialize the MCMC sampler\n",
    "initial = np.array([68, 0.3, -0.3, -1.3]) # H0=68, Om0=0.3, w0=-0.3, wa=-1.3\n",
    "ndim = len(initial)\n",
    "nwalkers = 500\n",
    "pos = initial + 1e-4 * np.random.randn(nwalkers, ndim) # provare con 1e-1\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=(data, Sigma_inv))\n",
    "\n",
    "# Run the MCMC sampler\n",
    "nsteps = 5000\n",
    "sampler.run_mcmc(pos, nsteps, progress=True)\n",
    "\n",
    "# Get the samples\n",
    "samples = sampler.get_chain(discard=300, thin=10, flat=True)\n",
    "\n",
    "# Plot the chains for each parameter\n",
    "fig, axes = plt.subplots(ndim, figsize=(10, 7), sharex=True)\n",
    "labels = [\"H0\", \"Om0\", \"w0\", \"wa\"]\n",
    "\n",
    "for i in range(ndim):\n",
    "    ax = axes[i]\n",
    "    ax.plot(sampler.chain[:, :, i], color=\"k\", alpha=0.3)\n",
    "    ax.set_ylabel(labels[i])\n",
    "    ax.set_xlabel(\"Step number\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot the results\n",
    "fig = corner.corner(samples, labels=[\"H0\", \"Om0\", \"w0\", \"wa\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of the corner plot\n",
    "\n",
    "* Histograms on the diagonal:\n",
    "\n",
    "These histograms show the marginal distribution for each parameter.\n",
    "\n",
    "* Contour plots:\n",
    "These plots represent the joint distribution between two parameters.\n",
    "If the plot is elongated an diagonal, the two parameters are correlated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comparison between real data and synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I want to calculate the distance modulus by the the result of the corner plot.\n",
    "\n",
    "Then I compute the theoretical distance modulus with my Cosmology model.\n",
    "\n",
    "In this way I can compare theoretical with the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My fiducial Cosmology is:\n",
    "fiducial_params = {\n",
    "    'H0': 68.0,\n",
    "    'Om0': 0.33,\n",
    "    'w0': -0.55,\n",
    "    'wa': -1.3\n",
    "}\n",
    "z = data['zcmb'].values\n",
    "\n",
    "# Compute theoretical magnitudes using the fiducial cosmology\n",
    "mu_theoretical = get_mu(z, **fiducial_params)\n",
    "\n",
    "# Calculate the percentiles\n",
    "percentiles = [16, 50, 84]\n",
    "labels = [\"H0\", \"Om0\", \"w0\", \"wa\"]\n",
    "\n",
    "# Get the parameter estimates\n",
    "estimates = np.percentile(samples, percentiles, axis=0) # 1-sigma uncertainties\n",
    "\n",
    "# Display the results\n",
    "for i, label in enumerate(labels):\n",
    "    q16, q50, q84 = estimates[:, i]\n",
    "    print(f\"{label}: {q50:.3f} (+{q84-q50:.3f}, -{q50-q16:.3f})\")\n",
    "\n",
    "# Check the shape and some basic statistics of the samples\n",
    "print(\"Samples shape:\", samples.shape)\n",
    "print(\"Mean of samples:\", np.mean(samples, axis=0))\n",
    "print(\"Standard deviation of samples:\", np.std(samples, axis=0))\n",
    "\n",
    "# Now I want to compute the distance modulus with this values and compare it with the theoretical distance modulus\n",
    "# Compute the distance modulus using the estimated parameters\n",
    "mu_estimated = get_mu(z, *estimates[1])\n",
    "\n",
    "# Plot the distance modulus\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(z, data['mb'], yerr=data['dmb'], fmt='o', label='Data')\n",
    "plt.plot(z, mu_theoretical, label='Theoretical')\n",
    "plt.plot(z, mu_estimated, label='Estimated')\n",
    "plt.xlabel('Redshift (z)')\n",
    "plt.ylabel('Distance Modulus (mu)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to understand the error of the previous code (vedere altro file: speremmu_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acc-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
